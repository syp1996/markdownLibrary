
视频讲述了GPT等大模型的工作原理及预测下一个词的原因，具体如下：
- **大模型的说话方式与人类的差异**：人类说话前会有大致的组织架构，而大模型是逐字生成，每次根据已有内容预测下一个字，类似接龙游戏或完形填空，虽与人类语言生成思路不同，但能生成逼真语言。
- **语言规律的本质与语言模型**：语言规律本质是理解token（词汇）在现实世界的含义及序列出现规律，传递信息和意义。计算语言学的核心问题是能否像计算机算数一样算出句子后接的正确词汇，语言模型便是描述这种规律的专业名词，语法是简单的语言模型。
- **传统语法建模语言的局限**：上世纪尝试用语法树让程序学会说话、翻译等任务，因语法仅规定基本规则，无法理解词语及连成有意义句子，且自然语言灵活、语法不断变化、网络流行语等新用法难用语法全面涵盖，该方法被淘汰。
- **统计语言模型的发展**：输入法联想是简单的统计语言模型，如one - gram model（每次看前一个相邻词/字）、two - gram model（看前两个字生成第三个字）等n gram model，基于上文长度用统计方法预测下一词。在GPT前，人类试过马尔可夫模型、循环神经网络、LSTM等预测算法，均属统计语言模型。
- **GPT的核心与强大原因**：GPT核心是根据上文预测下一个词，训练过程对应其“pre - trained”中的“p”，“G”是生成任务，“T”是用Transformer训练预测。Transformer作为强大的深度学习模型，可从训练数据学习输入输出规律，能处理大量上文信息进行next token prediction（预测下一个语速）。
- **预测下一词任务的深层意义**：看似简单的接龙游戏，大模型通过训练可学习到语言深层规律，理解词语含义、句子语法、文化及现实世界含义的合理性等。如根据语境预测合适形容词，通过大量文本学习语法结构、词语相关性等，实现有意义的句子生成和语言理解。
- **自回归生成与大模型的学习方式**：大模型完成学习后，可根据前文内容不断接词，将生成的输出作为新输入条件，这种方式叫自回归生成。它通过接触海量语料（书籍、新闻、网站等），无需人为标注，仅通过猜字学习语言规律，语料达TB量级。
- **大模型的运算量与训练成本**：大模型运算量巨大，如GPT3每生成一个字动用1750亿参数，进行十的15次方次浮点运算，训练需大量GPU支持，成本高昂，如GPT4训练花销至少上亿美金。
- **大模型的能力与局限**：大模型能理解语言深层规律，创造新内容、回答个性化问题，但存在局限，如无法说出恰好20字的话，专业知识可能胡编乱造（幻觉现象），因未真实接触世界，难检验信息真假、处理复杂数学因果。
- **大模型的训练环节与应用**：除基础训练外，还需在特定领域再训练以熟悉该领域词汇和知识，以及通过RLHF（人类反馈提升）让模型回话更对人类有帮助。大模型应用广泛，如作为知识渊博的助手解答问题、进行数据分析、生成智能体完成翻译等任务，像智谱清言在学习生活中可辅助学生和打工人。