<!--
 * @Author: Yunpeng Shi y.shi27@newcastle.ac.uk
 * @Date: 2025-06-20 13:50:54
 * @LastEditors: Yunpeng Shi y.shi27@newcastle.ac.uk
 * @LastEditTime: 2025-06-20 13:51:18
 * @FilePath: /markdown记录/一口气搞懂人工智能和神经网络.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->

- **人工智能的起源**：1956年的达特茅斯会议被视为人工智能的起点，该会议由麦卡锡、明斯基等科学家发起，探讨如何制造能学习和模拟人类智能的机器，如今的ALPHAGO、GPT等技术皆起源于该会议的研究。
- **智能的本质**：智能本质是通过收集信息，针对不同情景做出针对性反应，可看作输入到输出的函数对应关系，图灵测试便是基于此，若人无法区分与自己聊天的是AI还是人类，则认为该AI实现了人类智能。
- **人工智能的流派**：包括符号主义、机器学习（连接主义）。符号主义主张用符号逻辑推理模拟智能，其专家系统曾在疾病诊断等领域成功，但存在规则不清晰、能力上限低等缺陷；机器学习通过奖励或惩罚让机器自主学习，连接主义模拟大脑神经元及连接，感知机是早期模型，虽曾因无法解决异或问题陷入寒冬，但多层感知机（神经网络）经发展，理论上可拟合任何函数。
- **神经网络的发展**：1957年罗森布拉特造出感知机，能识别简单图片；1969年明斯基指出感知机无法解决异或问题，使连接主义陷入寒冬；后研究者通过增加神经元层数和数量，利用多层感知机解决异或问题，随着技术进步，出现了卷积神经网络、残差网络、Transformer等结构，推动了深度学习发展。
- **训练神经网络的方法**：核心是梯度下降和反向传播算法。梯度下降通过计算损失函数的梯度，让参数向损失减小的方向迭代；反向传播利用链式法则，从后往前层层传递梯度，计算复杂神经网络的梯度，从而优化参数，使模型学习数据规律。
- **神经网络的泛化能力**：泛化即举一反三，神经网络通过训练数据发现输入与输出的趋势和关联，虽未见过完全一样的数据，却能在未知情境给出合理预测，但其也存在不足，如可能因数据相关性误判，对抗样本会使其误判，且内部运作难以理解。
- **大语言模型（如GPT）**：以预测下一个词为核心，通过大量语料训练，学习语言规律，能理解语法、词语含义及现实逻辑，实现对话、写作等功能。其训练包括预训练和RLHF（人类反馈提升），但存在幻觉问题，对专业知识可能胡编乱造，也无法完全理解现实世界。
- **扩散模型与AI生成图片视频**：灵感源于扩散过程，通过逆向扩散（时光倒流），利用评分函数引导粒子从高斯分布恢复初始结构。在像素空间中，真实图片对应特定流行结构，扩散模型通过神经网络学习评分函数，逐步去噪，生成逼真图片，SORA在此基础上实现动态视频生成，虽在时间连贯性上有突破，但处理突兀变化场景仍有不足。
- **AI对就业的影响**：会对数据充足、模式固定的职业如文秘、插画、财务等造成冲击，但更可能改变工作性质而非完全取代。人类的创造力、情感和智慧难以被AI模拟，未来关键在于适应变化，与AI协同工作，提升自身技能。