<!--
 * @Author: Yunpeng Shi y.shi27@newcastle.ac.uk
 * @Date: 2025-06-20 13:23:51
 * @LastEditors: Yunpeng Shi y.shi27@newcastle.ac.uk
 * @LastEditTime: 2025-06-20 13:40:16
 * @FilePath: /markdown记录/GPT等大模型是怎么工作的？为啥要预测下一个词？.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
## 大模型的说话方式与人类的差异
人类说话前会有大概的结构和感觉，遵循语法和词汇含义；大模型则是每次根据已有内容预测下一个字，类似接龙和完形填空，生成思路与人类不同，但能生成逼真语言。

## 语言规律的本质与语言模型
言规律本质在于传递信息和意义，涉及理解 token 含义及序列规律；语言模型是计算语言学核心问题，语法是简单语言模型，能让人举一反三，但不足以描述语言本质，因自然语言灵活且语法会变化，上世纪基于语法的建模方法被淘汰。

## 统计语言模型的发展
输入法联想是简单语言模型，如 one gram model，看相邻词预测下一个词；还有 two gram model、n gram model 等，基于上文统计预测下一个词，被称作统计语言模型，颠覆语言学界认知，为 GPT 强大奠定基础，此前有马尔可夫模型、循环神经网络、LSTM 等预测算法，均属统计语言模型。

## GPT 的核心原理与强大原因
GPT 核心是根据上文预测下一个词，训练过程对应 pre - trained，G 是 generation（生成），T 是 transformer（用于训练预测）；transformer 强大，能从训练数据学习规律，大模型预测下一个词的信息是之前所有内容，即 next token prediction（预测下一个语速），通过此训练可学习到语言深层规律，实现理解语言、说话和解决问题。

## 自回归生成与大模型的学习方式
大模型通过见多识广，学习不同语境下应接的词，洞悉语言规律；把生成的输出作为新输入条件的方式叫自回归生成，可从某内容出发不断接词，解决问题。

## 大模型的训练数据与运算量
大模型训练靠接话尾，无需人为标注，需大量语料（如书籍、新闻、网站等），GBT 保守估计用 TB 量级语料，相当于上亿本《三国演义》；运算量大，如 GPT3 每说一个字动用 1750 亿参数，进行十的 15 次方次浮点运算，训练烧钱，需大量 GPU 支持。

## 自回归生成与大模型的训练
大模型完成学习后，根据前面内容不断往后接词，这种方式叫自回归生成；训练大模型需大量语料，其运算量大、训练成本高，如 GPT3 参数多、运算复杂。

## 大模型的能力与局限
大模型能理解语言深层规律、创造新内容、回答个性化问题，但存在幻觉现象，难以检验信息真假和处理复杂数学因果，未真实接触世界，对现实理解有距离。

## 大模型的进一步训练与应用
大模型训练有特定领域再训练和 RLHF（人类反馈提升）环节，使其更有用；像智谱清言等大模型已成为生活帮手，可用于搜集资料、数据分析、生成智能体等。