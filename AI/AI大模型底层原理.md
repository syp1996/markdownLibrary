<!--
 * @Author: Yunpeng Shi y.shi27@newcastle.ac.uk
 * @Date: 2025-06-20 10:45:33
 * @LastEditors: Yunpeng Shi y.shi27@newcastle.ac.uk
 * @LastEditTime: 2025-06-20 11:06:36
 * @FilePath: /markdown记录/AI大模型底层原理.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
## 决策式AI和生成式AI的区别
决策式 AI 根据已有数据做分析、判断和预测，如房价预测，在企业端常用于智能推荐、风控辅助决策等场景；生成式 AI 学习归纳已有数据后进行创造，基于历史数据模仿学习，通过缝合式创作生成全新内容，更符合人类理想中的智能化形态，AI 大模型生成文本时一个词一个词往外蹦是其原理决定的，大语言模型原理是根据上下文推理下一个文字。

## AI 大模型的训练与推理流程
包括预训练、微调和推理三个阶段。预训练采用无监督学习，基于大量知识库自主学习，如 GPT-3 以多个互联网文本为语料库，通过无监督学习得到基座模型，训练中涉及 token 化，token 是模型处理和理解文本的基本单元，不同模型分词器不同，token 化结果可能有差异；微调是在基座模型上进一步训练，改变内部参数，使其更适合特定任务，如训练擅长对话的模型需给基座模型看更多对话数据，此阶段类似监督学习；推理是模型根据训练后的能力生成内容。

## 大模型训练中的概率模型与向量嵌入
大模型训练本质是概率问题，通过训练数据集构建概率表，计算 token 跟随的概率来生成文本，早期模型因无法理解文本含义可能产生幻觉。向量嵌入是将 token id 转化为向量结构，原因是向量化表示数据维度更丰富，能捕捉词的语义和语法信息，相似词向量在空间中距离更接近，可通过计算向量距离度量对象相似性，如 GPT-3 向量长度为 12288，能包含大量信息。

## Transformer 架构与自注意力机制
Transformer 于 2017 年提出，成为自然语言处理主流架构，核心创新是自注意力机制，能捕捉序列内各元素关系，无论位置远近。通过计算词与词的相关性决定注意力权重，层数越多对文本理解越深刻，且是并行计算，如 GPT-3 有 96 层 Transformer，每一层关注不同信息，实现对句子和段落更深层次的理解，达到与人自由对话的智能化程度。
